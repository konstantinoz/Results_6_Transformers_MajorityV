Model,Accuracy,F1,Precision,Recall,MCC,kappa
"T5,GPT2,Pythia",0.8762626262626263,0.8760100651900036,0.8770445970948123,0.8762626262626263,0.8108578952828428,0.8102632299448508
"T5,Albert,GPT2",0.8737373737373737,0.8733846507421724,0.8742831031615924,0.8737373737373737,0.8071193676476801,0.8065196998123827
"T5,Albert,Pythia",0.8737373737373737,0.873194978138986,0.8741816836752685,0.8737373737373737,0.8074950337298185,0.8067293332161996
"DeBERTa,Albert,Pythia",0.8585858585858586,0.8578823184253911,0.8585767159268125,0.8585858585858586,0.7856571121847837,0.7850975375759514
"DeBERTa,GPT2,Pythia",0.8560606060606061,0.8559318579376818,0.8575782046198901,0.8560606060606061,0.7816197935528133,0.7809203055391096
"DeBERTa,T5,Pythia",0.8560606060606061,0.8563336833550588,0.8592378897332457,0.8560606060606061,0.781699784184377,0.7806264760479333
"Bart,GPT2,Pythia",0.8560606060606061,0.8544889642145925,0.8592886022289876,0.8560606060606061,0.7831564772618623,0.7803596450256889
"Bart,Albert,Pythia",0.8535353535353535,0.8518285844699588,0.8563171645469783,0.8535353535353535,0.7793960916629898,0.7766367137355584
"Bart,DeBERTa,GPT2",0.851010101010101,0.850095690854955,0.8545275992245688,0.851010101010101,0.774358630337656,0.7721607863787959
"Bart,T5,Pythia",0.851010101010101,0.850095690854955,0.8545275992245688,0.851010101010101,0.774358630337656,0.7721607863787959
"Bart,Albert,GPT2",0.851010101010101,0.849483236446923,0.8536788555376759,0.851010101010101,0.7751781496058691,0.7726662385428221
"DeBERTa,T5,GPT2",0.851010101010101,0.8513531246397248,0.8539713209536443,0.851010101010101,0.7739319540126622,0.7729843176120795
"DeBERTa,Albert,GPT2",0.851010101010101,0.8509629830547314,0.8523667951740673,0.851010101010101,0.773866655307485,0.773288309268747
"Bart,DeBERTa,Pythia",0.8484848484848485,0.8474375188241755,0.8514742104361582,0.8484848484848485,0.7705599440471684,0.7684345944681598
"Bart,T5,GPT2",0.8484848484848485,0.8477359587553763,0.8519760483211302,0.8484848484848485,0.7701916348188428,0.768177028451001
"Bart,DeBERTa,T5",0.8459595959595959,0.8453719750072803,0.8494445566223062,0.8459595959595959,0.7660371955317208,0.7641890704621332
"DeBERTa,T5,Albert",0.8459595959595959,0.846186763258872,0.8479376255401184,0.8459595959595959,0.7661371835048406,0.7655030481885605
"Bart,T5,Albert",0.8459595959595959,0.8447812916604419,0.8484691383318319,0.8459595959595959,0.7667816186219724,0.7647127578750511
"Bart,DeBERTa,Albert",0.8434343434343434,0.8421267690841266,0.845510662177329,0.8434343434343434,0.7630233590711478,0.7609952689679341
"Albert,GPT2,Pythia",0.8383838383838383,0.839045971783594,0.8469302550048735,0.8383838383838383,0.7582857577738294,0.7555838444624465
T5-base,0.8282828282828283,0.8284020490056532,0.8344790100311806,0.8321664894245541,0.7368147540373443,0.7367639008367874
GPT-2,0.8232323232323232,0.8229071092414757,0.8257129360284429,0.8304531490015362,0.7301761361741481,0.7300245432233433
DeBERTa-v3-base,0.8055555555555556,0.8044623774571364,0.8058618769145086,0.8152004805230613,0.7050294174266472,0.7041105461320498
BART-base,0.8055555555555556,0.805785090112912,0.8108857336037865,0.8107891212729923,0.7026763173733425,0.7023195876288659
ALBERT,0.7878787878787878,0.7857700999210433,0.7878835978835977,0.8042262397101106,0.6805872581847682,0.6786459540922791
Pythia,0.7348484848484849,0.732210735628929,0.7453361940204045,0.7432549529323723,0.599563245196926,0.5956196996810084
