Model,Accuracy,F1,Precision,Recall,MCC,kappa
"Bart,Albert,GPT2",0.8319307525847559,0.8316162613459337,0.8359545644767175,0.8319307525847559,0.7491582987043122,0.7470521666479065
"Bart,T5,GPT2",0.8294461809729903,0.8288802781040245,0.8330977365603274,0.8294461809729903,0.7455914386608358,0.7434119135227589
"Bart,DeBERTa,GPT2",0.8293660335016431,0.8291256543011405,0.8331791285719785,0.8293660335016431,0.7450674843285622,0.7431306126921257
"DeBERTa,Albert,GPT2",0.8277630840746975,0.8281981705127716,0.834048778327952,0.8277630840746975,0.7434387196161849,0.741158481242232
"DeBERTa,T5,GPT2",0.8275226416606556,0.8277967357036955,0.8334687969644037,0.8275226416606556,0.7431287590468525,0.7408413458183494
"DeBERTa,T5,Albert",0.8268013144185301,0.8269630166947497,0.832505903749298,0.8268013144185301,0.7420704059384508,0.7397830919444566
"Bart,T5,Albert",0.8267211669471828,0.8260005888553329,0.8300854878433197,0.8267211669471828,0.7415555176529705,0.7393614839588937
"Bart,DeBERTa,T5",0.826480724533141,0.8259343543079155,0.8298774698932668,0.826480724533141,0.7409516766127352,0.7389105937393357
"Bart,DeBERTa,Albert",0.8259998397050573,0.8256316083800948,0.8294127267733294,0.8259998397050573,0.73995698924814,0.7380825561354292
"Bart,GPT2,Pythia",0.8241564478640699,0.824123108649936,0.8274584017248205,0.8241564478640699,0.7365499779108363,0.7350568757559535
"Bart,Albert,Pythia",0.8207101065961369,0.8205859651116971,0.8236469153478371,0.8207101065961369,0.7312861702759387,0.729871009883359
"DeBERTa,GPT2,Pythia",0.8194277470545804,0.8202125144135011,0.8253209273431142,0.8194277470545804,0.7301469024867064,0.7283678340343902
"DeBERTa,Albert,Pythia",0.8180652400416767,0.8187530809261377,0.8236012130855604,0.8180652400416767,0.728042450006495,0.7263270727928424
"Bart,T5,Pythia",0.8163821431433839,0.8160148620948527,0.8187811057313913,0.8163821431433839,0.7248380258779803,0.7234330119997019
"DeBERTa,T5,Pythia",0.8156608159012583,0.8162112345906437,0.82060955407834,0.8156608159012583,0.7242960296522332,0.7227055878170766
"Bart,DeBERTa,Pythia",0.8153402260158692,0.8152019341218484,0.8178196844130623,0.8153402260158692,0.7229371623047518,0.7217147820580692
"T5,Albert,GPT2",0.8087681333653923,0.8102575707412807,0.815909406859801,0.8087681333653923,0.7150659029755878,0.7133327542235748
"T5,GPT2,Pythia",0.8001122064598862,0.8020423701796872,0.807927946231229,0.8001122064598862,0.7019202715155008,0.7002461279759706
"T5,Albert,Pythia",0.7985092570329406,0.8003847556400105,0.8059257454081,0.7985092570329406,0.6993742338129809,0.697823788734682
"Albert,GPT2,Pythia",0.7971467500200369,0.7979599567362698,0.7999154676540452,0.7971467500200369,0.6957005035068654,0.6952379173895642
DeBERTa-v3-base,0.7694958724052257,0.768926341884625,0.7718296778025658,0.7705629640979379,0.6524261225184391,0.6521860551350379
BART-base,0.7640458443536107,0.7648087258317342,0.7691273770931795,0.7641848115913129,0.6439472657293425,0.6436319072444344
GPT-2,0.7525847559509498,0.7524540400557961,0.7553774878545232,0.7534416379015121,0.6267249889903675,0.6266020562291861
T5-base,0.7481766450268494,0.7458495078972687,0.7466693170834479,0.7533148766284526,0.6226848030653026,0.6216067112944632
ALBERT,0.739440570649996,0.7390018337040859,0.7401167651397285,0.7434332500457841,0.6083454560226954,0.6080627152575159
Pythia,0.7264566802917368,0.7251176468529059,0.734117930089603,0.723514079237645,0.5876351046773631,0.5856930807136582
