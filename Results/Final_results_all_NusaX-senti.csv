Model,Accuracy,F1,Precision,Recall,MCC,kappa
"Bart,DeBERTa,Albert",0.9333333333333332,0.9332535415751024,0.9366739251531878,0.9333333333333332,0.9006895357562834,0.8987990824450142
"DeBERTa,T5,Albert",0.9333333333333332,0.932819505603956,0.9331466666666668,0.9333333333333332,0.8987379696698458,0.8983050847457628
"Bart,DeBERTa,GPT2",0.9333333333333332,0.932981480697496,0.9363567464114833,0.9333333333333332,0.9007305567043292,0.8986965624366854
"DeBERTa,Albert,GPT2",0.9333333333333332,0.9330452989878084,0.9332974910394266,0.9333333333333332,0.8987055625622234,0.8984135584037384
"DeBERTa,T5,GPT2",0.93,0.9293607574302882,0.9296976255463384,0.93,0.8936563332648291,0.8931786967801008
"Bart,DeBERTa,T5",0.93,0.9294845794376216,0.9325955555555556,0.93,0.8955457175627873,0.8935810810810811
"Bart,Albert,GPT2",0.93,0.9297667046860894,0.9328082670018154,0.93,0.8954647104552381,0.8936888288896389
"T5,Albert,GPT2",0.9266666666666666,0.9267667945678544,0.9280939577200964,0.9266666666666666,0.8884339717777358,0.8877837286406529
"DeBERTa,GPT2,Pythia",0.9266666666666666,0.9261415222590192,0.9262664679371162,0.9266666666666666,0.888488968913334,0.8881678160530017
"DeBERTa,T5,Pythia",0.9266666666666666,0.9258922922059248,0.9263028310502284,0.9266666666666666,0.8886006729009402,0.8880483088509685
"Bart,T5,Albert",0.9266666666666666,0.926271910763436,0.929007583474816,0.9266666666666666,0.8902665265898116,0.8885737439222042
"Bart,T5,GPT2",0.9266666666666666,0.9259788520015036,0.9288978895727412,0.9266666666666666,0.8903871264289981,0.8884607584668425
"Bart,DeBERTa,Pythia",0.9233333333333332,0.922768474942513,0.9252694559778558,0.9233333333333332,0.8850945186342384,0.8834538206877913
"DeBERTa,Albert,Pythia",0.9233333333333332,0.922927855910564,0.9229168873241624,0.9233333333333332,0.88336102418932,0.8831637231826879
"T5,Albert,Pythia",0.9233333333333332,0.9234561369714828,0.924805669431808,0.9233333333333332,0.8833294962145198,0.8826829890334098
"T5,GPT2,Pythia",0.9233333333333332,0.9233224043715847,0.9243090196078432,0.9233333333333332,0.883155355555548,0.882613133718952
"Bart,GPT2,Pythia",0.9233333333333332,0.922768474942513,0.9252694559778558,0.9233333333333332,0.8850945186342384,0.8834538206877913
"Bart,T5,Pythia",0.92,0.9192557047472302,0.9215915346086124,0.92,0.8799482279827825,0.8783290523185074
"Bart,Albert,Pythia",0.92,0.919563661422564,0.921723176168708,0.92,0.8798414213226732,0.8784522925248159
"Albert,GPT2,Pythia",0.9166666666666666,0.9167143794357964,0.9174544873842224,0.9166666666666666,0.8730651413660003,0.872693632984231
DeBERTa-v3-base,0.9033333333333333,0.903743920299854,0.895176636055266,0.897480881691408,0.8529138662441971,0.8528118021249239
T5-base,0.9,0.8995848890589067,0.895462497530722,0.893196131354026,0.8474900670368763,0.8473023413640991
BART-base,0.9,0.9005571960028694,0.8928350970017638,0.8975933423301846,0.8487882478389007,0.8480961382662706
ALBERT,0.8933333333333333,0.8932047939683508,0.8888522453006361,0.8883940620782725,0.8373197090633409,0.8372771035324429
GPT-2,0.8666666666666667,0.8663545722108561,0.8595173979618776,0.8597390913180387,0.7967460395011206,0.7966515285026774
Pythia,0.85,0.8418609706051567,0.8580059195886535,0.8205465587044535,0.7746893457618971,0.7671450255278046
