Model,Accuracy,F1,Precision,Recall,MCC,kappa
"Bart,Albert,GPT2",0.8022259321090707,0.8027737475678567,0.8051982387496055,0.8022259321090707,0.6840434798043863,0.6833835772075845
"Bart,T5,GPT2",0.8010573177518086,0.8015190516752815,0.8040198806447079,0.8010573177518086,0.6828509767717176,0.682117933790098
"T5,Albert,GPT2",0.8010573177518086,0.8016108237667758,0.8048416265381467,0.8010573177518086,0.6854378351595038,0.6844097851749402
"Bart,Albert,Pythia",0.7978853644963828,0.7985141387520106,0.8005483471131452,0.7978853644963828,0.6755380837321144,0.6750412246352249
"Bart,GPT2,Pythia",0.7978853644963828,0.798512828421277,0.8005663043388914,0.7978853644963828,0.6756517905778402,0.6751503019357808
"T5,Albert,Pythia",0.7974401780745687,0.7981263134655278,0.8015911423913842,0.7974401780745687,0.6787553283297425,0.6775914605181069
"DeBERTa,T5,GPT2",0.7973845297718419,0.7980051070918291,0.8029266717010284,0.7973845297718419,0.6813995716848912,0.6796943207134636
"Bart,T5,Albert",0.7971619365609349,0.7976655684720396,0.7997939561136815,0.7971619365609349,0.6761344751233573,0.6755555184828441
"Bart,DeBERTa,T5",0.7958820255982193,0.7963262989428643,0.7985193479202938,0.7958820255982193,0.6746815987643421,0.6740363650303959
"Bart,DeBERTa,GPT2",0.7958263772954924,0.796287902950678,0.7984292921030143,0.7958263772954924,0.6744294207668218,0.673814286561548
"T5,GPT2,Pythia",0.7955481357818587,0.7962116632125186,0.7994346909667035,0.7955481357818587,0.6756863339576444,0.6746163746949777
"DeBERTa,Albert,GPT2",0.7954368391764052,0.7961352969297322,0.8010064958660466,0.7954368391764052,0.6779657693515698,0.6762737725173733
"DeBERTa,T5,Albert",0.7951585976627713,0.795812644004889,0.8005280855866538,0.7951585976627713,0.6776223950024303,0.6759955775077908
"DeBERTa,Albert,Pythia",0.7914858096828047,0.7923900013985222,0.7976669552562131,0.7914858096828047,0.6709330803561977,0.6690376766285733
"Bart,DeBERTa,Albert",0.7914301613800779,0.7919179571292687,0.7937109169067477,0.7914301613800779,0.6669350630431655,0.6664593529693087
"Bart,T5,Pythia",0.7912632164718976,0.7918235039768561,0.793405991512129,0.7912632164718976,0.6652019206788107,0.6648509326212919
"DeBERTa,GPT2,Pythia",0.7907623817473567,0.7916456702036205,0.7967292697682564,0.7907623817473567,0.6698309020812725,0.6680278628779259
"DeBERTa,T5,Pythia",0.7890929326655537,0.7899507328215795,0.7947477388337971,0.7890929326655537,0.6670978410692686,0.6654096699254846
"Bart,DeBERTa,Pythia",0.784974958263773,0.7854790593139084,0.7866982495698966,0.784974958263773,0.6551559596924005,0.6549109466409623
"Albert,GPT2,Pythia",0.7754034501947691,0.7767376696884636,0.7810116601916306,0.7754034501947691,0.6384883410589347,0.6374767743853733
DeBERTa-v3-base,0.7260434056761269,0.72606659358409,0.7209816000058499,0.7224704296364267,0.5651934920101819,0.5651890189934656
T5-base,0.7244852531997774,0.7248717357998892,0.7182089669541902,0.7188788359783441,0.562442770558379,0.5622735898069173
BART-base,0.7179187534780189,0.718226303214572,0.7153132816284514,0.7082215400974453,0.5506260140178465,0.5494634287823005
GPT-2,0.7068447412353923,0.7070517955466762,0.7007412370442446,0.6980772605730109,0.533400273448531,0.5331854512977638
ALBERT,0.6957707289927657,0.6957038175741224,0.6940117423112886,0.6789276955085177,0.512941816775431,0.5115893560692633
Pythia,0.6767390094602115,0.6733393922850481,0.6865586473864904,0.6385116408850546,0.4767580941130072,0.47074068544064995
